\section{Elección de una red neuronal}
Para la elección de la red neuronal se tomará en cuenta un estudio realizado por el equipo de YOLO, (\cite{persona}), este equipo realizo pruebas con diferentes redes neuronales, para poder demostrar el gran rendimiento que tiene la red YOLO, estas pruebas fueron echas en la misma computadora, el resultado de estas pruebas se puede observar en la Figura \ref{fig:labeling3} que da como ganador la red neuronal YOLO.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{imagenes/persona9.png}
	\caption{Comparación de YOLO y otros detectores de objetos de última generación.}
	\source{YOLOv4: Optimal Speed and Accuracy of Object Detection. (2020)}
	\label{fig:labeling3}
\end{figure}

Estas pruebas fueron realizadas haciendo el uso de la base de datos \say{coco}, que recopila una gran base de datos que facilitan la detección de diferentes objetos. Con estos resultados nos muestran que la red neuronal YOLO es la mejor opción, ya que detecta los objetos con mayor velocidad y con un mayor porcentaje de acierto. 

\section{Entrenamiento de la red neuronal}
En este capítulo se desarrollarán los diferentes pasos para lograr entrenar una red neuronal, para que sea capaz de detectar personas ya ser en imágenes o en videos tanto grabados como en tiempo real.

 \subsection{Base de datos}
Uno de los primeros pasos para entrenar una red neuronal es tener una base de datos, la cual está compuesta por imágenes (donde aparezca el objeto a detectar) y un archivo que indique la posición del objeto a detectar en la imagen.
\\ Para ello se consiguió 400 imágenes donde aparezcan personas y con ellos realizar un procedimiento de etiquetado, en la Figura \ref{fig:labeling} se observa este proceso. Esto se logra mediante el programa LabelImg que te permite etiquetar la posición de una persona en una imagen.


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.25]{imagenes/persona1.png}
	\caption{Proceso de etiquetado para generar los archivos necesarios para el entrenamiento.}
	\source{Elaboración propia}
	\label{fig:labeling}
\end{figure}

Este proceso se realiza con cada imagen, para así obtener un archivo que especifica la posición de dónde se encuentra la \say{persona} en este caso. En la Figura \ref{fig:labeling2} se observa la manera en la que se guardan esos datos. 

\clearpage

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{imagenes/persona2.png}
	\caption{Coordenadas obtenidas por el programa LabelImg al etiquetar a un objeto.}
	\source{Elaboración propia}
	\label{fig:labeling2}
\end{figure}

Esta recopilación de datos se puede realizar más fácil gracias a Google, facilitando esta tarea permitiéndote usar su propia base de datos donde aproximadamente tiene cuatro millones de imágenes, Figura \ref{fig:labeling5}, el cual ya fueron etiquetados varios objetos como ser personas, autos, aviones, relojes y otros mas.



\begin{figure}[h]
	\centering
	\includegraphics[scale=0.2]{imagenes/persona5.png}
	\caption{Base de datos de Google para el entrenamiento de redes neuronales.}
	\source{Elaboración propia}
	\label{fig:labeling5}
	\end{figure}

\subsection{Preparación del ambiente}

Para entrenar una red neuronal se necesita muchos recursos en una computadora sobre todo en la área del GPU, ya que todos los cálculos que realiza son procesados de manera más efectiva en esa área. 
Por ese motivo se decidió usar un servicio de Google que es Colab, el cual te permite conectarse a sus servidores y poder entrenar la red de forma gratuita. 
\\

Colab es un editor de texto online, que te permite escribir tu código y probarlo sin la necesidad de instalar programas en tu computadora, pero será necesarios instalarlos en tu editor online. El primer paso es instalar Darknet, el cual te permite trabajar con YOLO.

\clearpage

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{imagenes/persona3.png}
	\caption{Frameworks necesarios para que funcione Darknet.}
	\source{Elaboración propia}
	\label{fig:labeling42}
\end{figure}

 Como se observa en la Figura \ref{fig:labeling42}, Darknet necesita diferentes frameworks para que funcione. Estos se instalan con la ayuda del comando \textit{pip}, el cual es un sistema de gestión de paquetes de Python. Se procederá a instalar estos paquetes mediante el comando que se aprecia en la Figura \ref{fig:labeling43}, ya que estos requisitos están guardados en un archivo \say{.txt} en el mismo Darknet.
 
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{imagenes/persona4.png}
	\caption{Comando de instalación de paquetes.}
	\source{Elaboración propia}
	\label{fig:labeling43}
\end{figure}


\subsection{Personalizar una red neuronal}

En esta etapa, se tiene que modificar diferentes archivos para que la red pueda detectar a personas. Estos archivos por modificar son los parámetros que definen la red neuronal, en Darknet existen 2 redes principales que son Yolo y Tiny-Yolo. La gran diferencia entre estos son la cantidad de capas que poseen, esto llega a afectar el rendimiento en una detección. 
\vspace{5mm}

Por eso mismo se decidió usar Tiny-Yolo, ya que con pruebas realizadas dio un mejor rendimiento en el apartado de frames por segundo (fps), al momento de trabajar con videos, esto es muy importante porque la detección se realizará en tiempo real. Los resultados de la prueba se apreciará mejor en la Figura \ref{fig:labeling4}.

\clearpage

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{imagenes/persona10.png}
	\caption{Resultados de una prueba de rendimiento entre YOLO y Tyni-Yolo.}
	\source{Elaboración propia}
	\label{fig:labeling4}
\end{figure}

Con estos resultados se modificara el archivo \say{yolov4-tiny.cfg}, y los datos a cambiar dependerán mayormente a la cantidad de clases al cual se entrenara la red neuronal, estos datos son los que están marcados en la Figura \ref{fig:labeling6}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{imagenes/persona6.png}
	\caption{Se define el learning rate.}
	\source{Elaboración propia}
	\label{fig:labeling6}
\end{figure}

El dato marcado \textit{learning rate} es el que determina el ritmo de aprendizaje, este dato entre más bajo podría aprender mas pero  eso alarga el periodo de entrenamiento y aumenta la probabilidad de que se detenga el entrenamiento por un error, en este apartado no hay que exagerar porque un dato muy bajo simplemente lo hará mas lento y no ayudara en el entrenamiento ya que ni se podría realizar.

El siguiente es \textit{max batches}, el quese encarga de limitar el número de pasos máximos que realizará en el entrenamiento. Este número es obtenido mediante una simple fórmula, pero solo es aplicada cuando se trabajará con 3 o más clases de detección, caso contrario simplemente se tendrá que colocar \say{6000}. En \textit{step} se tómara el 80\% y 90\% de \say{max batches} respectivamente.

\begin{equation}
	max\_batches = \#\operatorname{clases} * 2000
\end{equation}

Los otros datos por modificar son los que se aprecian en la Figura \ref{fig:labeling7}, que consiste la cantidad de filtros y especificar cuántas clases tendrá que aprender la red neuronal.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{imagenes/persona7.png}
	\caption{Se establece la cantidad de filtros a usarse.}
	\source{Elaboración propia}
	\label{fig:labeling7}
\end{figure}

Para calcular la cantidad de filtros se tendrá que usar otra fórmula simple, este dato obtenido se tendrá cambiar en todos los filtros que se encuentra en el documento \say{yolov4-tiny.cfg}.

\begin{equation}
	filtros = (\#\operatorname{clases}+5)*3
	\label{eq:persona}
\end{equation}

Con eso finaliza los cambios en el archivo \say{yolov4-tiny.cfg}, pero aún falta editar otros 2 archivos mas el cual uno definirá el nombre de la clase y en el otro se definirán la ubicación de los archivos necesarios para entrenar y la locación de donde se guardara la red entrenada, esto se aprecia en la Figura \ref{fig:labeling45}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{imagenes/persona8.png}
	\caption{Se detalla la cantidad de clases y nombres que se usarán para entrenar la red neuronal.}
	\source{Elaboración propia}
	\label{fig:labeling45}
\end{figure}

\subsection{Inicio de entrenamiento}

Para esta etapa ya se tiene todos los archivos necesarios para iniciar el entrenamiento dentro del ambiente de Google Colab, el cual facilita este procedimiento. Para comenzar se tiene que ejecutar los siguientes scripts que apareceén en la Figura \ref{fig:labeling46}, el cual creará unos archivos basados con la cantidad de clases por el cual vamos a entrenar y la cantidad de imágenes con el cual entrenara.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{imagenes/persona11.png}
	\caption{Comando para generar archivos que guiaran el entrenamiento.}
	\source{Elaboración propia}
	\label{fig:labeling46}
\end{figure}

Con todo esto sólo queda correr otro comando el cual iniciará el entrenamiento, este se podrá ver en la Figura \ref{fig:labeling47}. Éste entrenamiento normalmente lleva entre 2 a 3 horas aproximadamente.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{imagenes/persona12.png}
	\caption{Comando para iniciar el entrenamiento.}
	\source{Elaboración propia}
	\label{fig:labeling47}
\end{figure}

Al terminar el entrenamiento ya se tendrá una red neuronal funcional, capaz de detectar personas. Un resultado del entrenamiento se observa en la Figura \ref{fig:labeling48}, donde se ven marcadas una cantidad aceptable de personas que aparecen en la imagen.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{imagenes/persona14.png}
	\caption{Resultado del entrenamiento realizado.}
	\source{Elaboración propia}
	\label{fig:labeling48}
\end{figure}

\section{Diseño de la interfaz}
Para el desarrollo de interfaces se utilizó la herramienta Balsamiq Wireframes con 2 pantallas principales, el cual son: detección de personas y ventana de alarma. 
\subsection{Detección de personas}

Esta es la pantalla principal de la aplicación, en el cual se observa las detecciones que se esta realizando, el límite de detección y la cantidad de personas que aparecen en el video, esto se apreciará mejor en la Figura \ref{fig:labeling30}.
\clearpage
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{imagenes/imagen1.png}
	\caption{Pantalla principal de la aplicación NeoVision.}
	\source{Elaboración propia}
	\label{fig:labeling30}
\end{figure}

\subsection{Pantalla de alerta}

Esta pantalla solo aparecerá cuando la alarma se active, lo que significa, es que se llegó a sobrepasar el tiempo máximo de espera de una persona en la fila, el diseño de esta pantalla se puede ver en la Figura \ref{fig:labeling31}.


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{imagenes/imagen2.png}
	\caption{Mockup de la pantalla de alerta.}
	\source{Elaboración propia}
	\label{fig:labeling31}
\end{figure}

\section{Diagrama de flujo}
En este apartado se mostrará un diagrama de flujo en el que se representa los principales procesos de este proyecto, cuales son detectar a personas y dependiendo del tiempo de espera notificar con una alerta, este diagrama se observa en la Figura \ref{fig:labeling33}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{imagenes/NeoVision.png}
	\caption{Diagrama de flujo explicativo sobre la función de la aplicación NeoVision.}
	\source{Elaboración propia}
	\label{fig:labeling33}
\end{figure}

\section{Diagrama C4Model}
En este apartado se dará conocer un esquema siguiendo el modelo C4Model, el cual se va explicando de manera general hasta llegar a lo mas específico, empezando por el contexto de la situación, sus contenedores, componentes y por último un diagrama de todas las clases.
\clearpage
\subsection{Contexto}
En este apartado se observará las diferentes interacciones que tiene la aplicación, esto se apreciará mejor en la Figura \ref{fig:labeling32}.


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{imagenes/contexto.png}
	\caption{Diagrama de contexto que representa las interacciones del sistema.}
	\source{Elaboración propia}
	\label{fig:labeling32}
\end{figure}

\subsection{Contenedores}

En esta sección se observa mas a detalle los sistemas usados para que la aplicación funcione, esto se ve en la Figura \ref{fig:labeling34}.
\clearpage
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{imagenes/contenedor.png}
	\caption{Diagrama de contenedores que representa las diferentes funciones de los sistemas.}
	\source{Elaboración propia}
	\label{fig:labeling34}
\end{figure}


\section{Descripción de la aplicación}



Para comenzar el desarrollo de la aplicación primeramente se tiene que crear una ambiente virtual de python en la computadora, ya que se necesita instalar ciertos paquetes que podrían entrar en conflicto con algunas aplicaciones ya instaladas y también genera un mayor orden.
La herramienta ideal para realizar esto es VirtualenvWrapper, ya que es fácil de instalar y sobre todo es simple de ingresar al ambiente virtual, sobre todo cuando se compara con otras herramientas. Esta diferencia aprecia mejor en la Figura \ref{fig:labeling9}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{imagenes/persona15.png}
	\caption{Diferencia entre iniciar un ambiente virtual de Python entre VirtualWrapper y Virtualenv.}
	\source{Elaboración propia}
	\label{fig:labeling9}
\end{figure}



Con el ambiente virtual se procede a instalar los diversos paquetes necesarios, donde los principales son Tensorflow y OpenCV. Teniendo estos recursos instalados, se tendrá que convertir la red neuronal entrenada a un formato que Tensorflow soporte. Para ello usaremos un código disponible hecho por la misma comunidad, que esta hecho en Python, en la Figura \ref{fig:labeling10} se muestra el comando para convertir la red.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{imagenes/persona16.png}
	\caption{Comando para convertir la red neuronal.}
	\source{Elaboración propia}
	\label{fig:labeling10}
\end{figure}

 Para realizar la aplicación se hará uso del editor PyCharm, porque es gratis y tiene una interfaz intuitiva al momento de usarlo.
 El primer paso al comenzar la aplicación es comenzar a importar todos los paquetes que se necesitará para que funcione, la manera en la que se importa los paquetes se podrá observar en el siguiente código \ref{23}.
\begin{lstlisting}[language=Python,caption={Manera en la que se importan los diferentes paquetes al programa.},captionpos=b,label=23]
	import os 
	import subprocess
	from datatime import datetime
	import tensorflow as tf
	import time
\end{lstlisting}

Con todo esto ya se puede dar comienzo para que la aplicación pueda leer los videos mediante OpenCV y así iniciar el detector para que encuentre las personas de un video.

\subsection{Agregar el tracker a la detección}

Por sí solas la detección que se puede hacer no brinda la información suficiente como para contar las detecciones o corregir errores que se pueden generar. Uno de estos errores es que como en un video hay objetos en movimiento simplemente puede desaparecer las detecciones, debido a que por el movimiento no se llega a reconocer al objeto, esto se puede ver en la Figura \ref{fig:labeling12}.



\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{imagenes/deteccionvstracker.png}
	\caption{Comparación entre usar solo la detección y usar tracker con detección.}
	\source{Elaboración propia.}
	\label{fig:labeling12}
\end{figure}

\clearpage
Cómo se ve en la figura anterior, no se marca todos los objetos en el lado que no utiliza \textit{tracker}, ya que justo en ese \textit{frame} no lo identifica bien y se pierde. Pero en otra parte del mismo video se ve que en la Figura \ref{fig:labeling13}, desaparece el objeto detectado, por otra parte el que utiliza el \textit{tracker} no pierde ninguna detección.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{imagenes/deteccionvstracker2.png}
	\caption{Error al solo usar la detección en un proyecto.}
	\source{Elaboración propia.}
	\label{fig:labeling13}
\end{figure}


Pero el problema mas importante es que cuando una persona se sale del enfoque de la cámara o es cubierto por algún objeto, la detección se pierde y cuando vuelve a aparecer la app lo detecta como si fuera una nueva persona. 
Esto se podrá solucionar incorporando a la detección un \textit{tracker}, mediante esto se podrá generar ID a cada detección y con eso se podrá diferenciar entre detecciones, ya que todos tendrán un identificador que permitiría contar a las personas detectadas.
El \textit{tracker} a utilizar es DeepSort, el cual es de uso libre, para que funcione correctamente se tiene que usar una red pre-entrenada que ayuda al proceso del rastreo.

\subsection{Agregar contador y límite de detección}

Para que se pueda contar la cantidad de personas detectas se usara una función de Deepsort el cual genera una lista de la detecciones y mediante OpenCV se podrá mostrar en la pantalla del mismo video como se ve en la Figura \ref{fig:labeling14}.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.35]{imagenes/persona19.png}
	\caption{Contador de personas en un video.}
	\source{Elaboración propia}
	\label{fig:labeling14}
\end{figure}

El límite se encargará de reiniciar el contador y los cálculos de tiempo máximo de espera de un cliente en una fila. En la Figura \ref{fig:labeling35} se aprecia cómo se muestra el límite creado, esta línea es solo una representación visual, ya que las coordenadas de la línea son los importantes al momento de trabajar en la aplicación.


\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{imagenes/persona20.png}
	\caption{Línea que limita las detecciones en un video.}
	\source{Elaboración propia}
	\label{fig:labeling35}
\end{figure}

Esto se logra gracias a OpenCV, ya que con él se dibujara una línea, el cual representa el límite para la detección y cuando un objeto cruce la línea ya no será tomando en cuenta. Esto funciona porque por cada detección genera unas coordenadas de la posición del objeto, teniendo esto se sabe cuándo cruza la línea.

\subsection{Alarma al superar el tiempo máximo}

Este apartado se encarga de informar cuando se sobrepase el tiempo máximo de espera definido, para saber esto se creó una lista donde se registra la hora en la que se detectó la persona, en la Figura \ref{fig:labeling16} se observa como esta compuesta la lista, la cual guarda datos necesarios para realizar la alarma y el límite de detección.

\clearpage

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{imagenes/persona21.png}
	\caption{Lista que guarda datos cada vez que se realiza una nueva detección.}
	\source{Elaboración propia}
	\label{fig:labeling16}
\end{figure}

Teniendo esta lista podemos registrar la hora en la que fue detectado una persona y con ello al tener una lista donde se detectaron 3 personas nos permite calcular un tiempo promedio de espera y con ese dato se podrá comparar con el tiempo máximo de espera.
Si este tiempo promedio de espera es mayor que el tiempo máximo, se procederá a notificar mediante un \say{popup} que el cliente esta esperando mucho y que se recomienda abrir una nueva caja.
\\

Otra manera en la que se puede activar la alarma es cuando por un determinado tiempo ni una persona llegó a cruzar el límite definido, de igual manera que en el anterior caso se activará un \say{popup} notificando un problema al encargado para determinar si lo ve necesario abrir una caja o no. En la Figura \ref{fig:labeling17} se observa el diseño del \say{popup} y el mensaje que da.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{imagenes/persona22.png}
	\caption{Alerta popup que se activa cuando se alcanza el tiempo máximo de espera.}
	\source{Elaboración propia}
	\label{fig:labeling17}
\end{figure}

\clearpage

\subsection{Crear una base de datos}

Con este apartado se está utilizando una base datos para registrar la hora exacta cuando salte una alarma. Esto sirve para poder ver cuándo son las horas críticas y así buscar una solución que mejoraría la atención.
Para la base de datos se usa Mongodb, el cual te permite guardar los datos tanto de manera local como en la nube y es una herramienta gratuita.

En este caso como aplicación fue probada en un MacBook Pro 2014, la instalación de Mongodb variaráa en una máquina Windows. Simplemente para instalar la base de datos se tendrá que correr el siguiente comando que aparece en la Figura \ref{fig:labeling18}, estos comandos se tienen que hacer correr en la terminal.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{imagenes/mongodc.png}
	\caption{Comando de instalación en Mac de MongoDb.}
	\source{Elaboración propia}
	\label{fig:labeling18}
\end{figure}

El dato que se guardará es la hora en la que la alarma se active, esto se logra con los siguientes comandos que aparecen en el siguiente codigo \ref{24}, estos tienen que estar dentro de la aplicación creada.
 \begin{lstlisting}[language=Python,caption={Comando para guardar información en la base de datos.},captionpos=b,label=24]
post = {"data": datetime.now()}
alarms = db.alarms
alarms.insert_one(post)
DETECTION_EVENTS.clear()
\end{lstlisting}

En este caso la información de la base de datos se almacena de manera local, lo que significa que cada vez que se inicie la aplicación se tendrá que activar Mongodb, esto se logra con el siguiente comando que aparece en la Figura \ref{fig:labeling20}, de la misma manera se tendrá que desactivar cuando ya no se utilice la aplicación. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{imagenes/persona25.png}
	\caption{Comandos para iniciar y detener la base de datos.}
	\source{Elaboración propia}
	\label{fig:labeling20}
\end{figure}


\section{Funcionamiento de la aplicación}

En este apartado se explicara el funcionamiento de la aplicación, que permitirá poder obtener datos en tiempo real de cuanto una persona en promedio espera hasta ser atendido en la caja.

\subsection{Proceso de iniciar la aplicación}

Para que la aplicación funcione se tiene que correr mediante terminar, utilizando el siguiente comando que aparece en la Figura \ref{fig:labeling15}, el comando  correrá el código escrito y se agregan ciertos parámetros que necesita Darknet, el cual definirán el funcionamiento.

\begin{figure}[h]
	\centering
	\includegraphics[width=150mm, height=45mm]{imagenes/persona26.png}
	\caption{Comando para que funcione la aplicación, con los parámetros necesarios.}
	\source{Elaboración propia}
	\label{fig:labeling15}
\end{figure}

Estos parámetros definen con qué tipo de video se trabajará, si será un video local, usar una cámara convencional o usar una cámara IP. Otro parámetro importante es definir si los resultados del análisis se guardara en un video.	

\subsection{Proceso de detección y cálculo de tiempo de espera promedio}

Cuando la aplicación ya está iniciada comenzara a realizar la detección en cada Frame del video esperando detectar alguna persona. 
Ahora, cuando llega a detectar una persona guardara los siguientes datos: la hora en la que fue detectada, el número de detección, la posición del objeto detectado.
\vspace{5mm}

Paralela a la acción anterior, también comienza a ejecutarse continuamente 2 funciones que calcularán el tiempo promedio de espera de un cliente.
Una función ira calculando el tiempo promedio de espera dependiendo de la cantidad de personas que están haciendo fila, esto se realiza gracias a los datos que se almacenan en una lista, esta lista se observa en la Figura \ref{fig:labeling16}. Esta función que se puede ver en el siguiente código \ref{25}, realiza un procedimiento de crear una lista en el que se suman la diferencia del tiempo en el que fueron detectados los objetos.
\begin{lstlisting}[language=Python,caption={Función que suma la diferencia de tiempo entre cada detección en segundos.},captionpos=b,label=25]
def calculate_average_waiting_time_in_seconds():
    datetime_diff_in_seconds = []
    for oldest_event, newest_event in zip(DETECTION_EVENTS,
    	 DETECTION_EVENTS[1:]):
        diff = oldest_event['datetime_event'] - 
        	newest_event['datetime_event']
        datetime_diff_in_seconds.append(abs(diff.total_seconds()))
    return sum(datetime_diff_in_seconds)
\end{lstlisting}

Teniendo este dato, se procede a calcular el tiempo promedio de espera, para ello se ejecutara diversos cálculos para poder obtenerlo, esto se aprecia en el siguiente código \ref{26}.
\begin{lstlisting}[language=Python,caption={Calculo para la obtención del tiempo promedio de espera.},captionpos=b,label=26]
        if DETECTION_EVENTS:
            waiting_time_in_seconds = 
            	calculate_average_waiting_time_in_seconds()
            last_number_of_people_detected =
            	 get_last_number_of_people_detected()
            average_waiting_time_in_seconds = 
            	waiting_time_in_seconds / 
            			last_number_of_people_detected
            print("AVG: {}".format(average_waiting_time_in_seconds))
            diff2 = DETECTION_EVENTS[0]["datetime_event"] - 
            		datetime.now()
            max_time = abs(diff2.total_seconds())
\end{lstlisting}

La segunda función que se encarga de activar la alarma consiste en saber cuándo un objeto llega a cruzar el limite dibujado anteriormente. Ahora, si un objeto detectado no llego a cruzar este limite en un tiempo determinado, se llega a considerar que hay un problema en la fila y activara la alarma.

\begin{lstlisting}[language=Python,caption={Function para calcular el objeto mas cercado al límite.},captionpos=b,label=27]
def create_list_of_closest_person_detected():
    list_closest_person_detected = [0, 0]
    for closest_detecttion in DETECTION_EVENTS:
        person_closes = closest_detecttion[0]
        		['detection_position', 0]
        list_closest_person_detected.append(person_closes)
        max_value = np.max(list_closest_person_detected)
    return max_value
\end{lstlisting}
\clearpage

Con el anterior código \ref{27}, se llega a saber cual es la persona que este al frente de la fila y con ellos determinar si se tarda mucho en ser atendido, ya que eso se sabe cuando cruza el límite establecido.


Los cálculos de estas funciones serán reniciadas cada vez que salte la alarma o cuando sea atendida una persona.
La alarma suena cada vez que se supere el tiempo máximo de espera, el cual se define al comienzo de la aplicación. 
\vspace{5mm}

Al sonar la alarma se inician 2 procesos importantes, el primero se encargará de generar un \say{popup} con un mensaje avisando que es recomendable abrir una nueva caja. Por otra parte, la otra función se encarga de registrar de la hora en la que se activó la alarma, para así tener un registro para observar cuales son los periodos del día con más problemas en atender al cliente. Con esa información se puede buscar una solución permanente que mejore la experiencia del cliente al tener menos tiempo de espera.

























